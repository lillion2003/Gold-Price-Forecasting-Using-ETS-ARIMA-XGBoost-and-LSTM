{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Grid Search -\n",
    "นำ LSTM มา Grid Search โดยใช้วิธี Rolling Window CV แต่ละ Fold (รอบการประเมิน) จะถูกเทรนด้วยวิธี multi step direct forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ตั้งค่าและนำเข้า Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup & Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import itertools\n",
    "import time\n",
    "import copy\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device Configuaration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "\n",
    "# Settings\n",
    "SEED = 42\n",
    "N_JOBS = 64\n",
    "VALID_SIZE = 155\n",
    "\n",
    "N_SPLITS = 5\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## โหลดข้อมูล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Data Loaded: n=773, n_train_full=618\n",
      "Diff-1 Series Length (for CV): 617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5205/509359041.py:8: UserWarning: Parsing dates in %m/%d/%Y format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Loading & Preprocessing\n",
    "sheet_id = \"1-hzX_qRFjS7TIhWkTsrWPx7M_cFATCQxvWlRmo97Wac\"\n",
    "sheet_gid = \"0\"\n",
    "csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&gid={sheet_gid}\"\n",
    "\n",
    "print(\"Loading data\")\n",
    "data = pd.read_csv(csv_url)\n",
    "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)\n",
    "data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Parameters\n",
    "n = len(data)\n",
    "n_train_full = 618 \n",
    "y = data['Y'].values\n",
    "dy = np.diff(y)\n",
    "dy_train_full = dy[:n_train_full - 1]\n",
    "\n",
    "print(f\"Data Loaded: n={n}, n_train_full={n_train_full}\")\n",
    "print(f\"Diff-1 Series Length (for CV): {len(dy_train_full)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ออกแบบการ Cross Validation ตามแนวทาง Rolling Window CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CV process\n",
    "\n",
    "def create_sequences_direct(series, window, horizon, end_index):\n",
    "    \"\"\"\n",
    "    Creates X, y for Direct strategy at specific horizon h.\n",
    "    \"\"\"\n",
    "    max_start = end_index - window - horizon + 1\n",
    "    if max_start < 0:\n",
    "        # Not enough data for this h\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(max_start + 1):\n",
    "        seq_x = series[i : i + window]\n",
    "        target = series[i + window + horizon - 1]\n",
    "        X.append(seq_x)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def make_cv_splits(n_seq, valid_size=155, n_splits=5):\n",
    "    \"\"\"\n",
    "    Rolling Overlap CV splits \n",
    "    \"\"\"\n",
    "    \n",
    "    usable = n_seq\n",
    "    if usable <= valid_size + 1:\n",
    "        raise ValueError(\"usable seq <= valid_size\")\n",
    "        \n",
    "    train_size = int(np.floor((usable - valid_size) * 3/4))\n",
    "    \n",
    "    start_min = 0\n",
    "    start_max = usable - (train_size + valid_size)\n",
    "    \n",
    "    if start_max < start_min:\n",
    "        raise ValueError(\"start_max < start_min\")\n",
    "        \n",
    "    if n_splits == 1:\n",
    "        step_s = 0\n",
    "    else:\n",
    "        step_s = max(1, int(np.floor((start_max - start_min) / (n_splits - 1))))\n",
    "        \n",
    "    splits = []\n",
    "    for k in range(n_splits):\n",
    "        s = start_min + k * step_s\n",
    "        tr_start = s\n",
    "        tr_end = s + train_size\n",
    "        va_start = tr_end\n",
    "        va_end = tr_end + valid_size\n",
    "        \n",
    "        splits.append((np.arange(tr_start, tr_end), np.arange(va_start, va_end)))\n",
    "        \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ออกแบบโครงข่ายประสาทเทียมแบบ LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Definition\n",
    "\n",
    "class LSTMDirect(nn.Module):\n",
    "    def __init__(self, lstm1_units, lstm2_units, dense1_units, dense2_units, window_size):\n",
    "        super(LSTMDirect, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=lstm1_units, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=lstm1_units, hidden_size=lstm2_units, batch_first=True)\n",
    "        self.fc1 = nn.Linear(lstm2_units, dense1_units)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(dense1_units, dense2_units)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(dense2_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function ดำเนินการ Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Grid Search\n",
    "# 1 Combination * 5 Folds * 155 Horizons = 775 Models trained.\n",
    "\n",
    "def train_model_for_horizon(train_series, h_target, w, params, end_idx_train, device):\n",
    "    \n",
    "    X, y = create_sequences_direct(train_series, w, h_target, end_index=len(train_series)-1)\n",
    "    \n",
    "    if len(X) == 0: return None\n",
    "    \n",
    "    X_t = torch.tensor(X.reshape(-1, w, 1), dtype=torch.float32).to(device)\n",
    "    y_t = torch.tensor(y.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "    \n",
    "    dataset = TensorDataset(X_t, y_t)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = LSTMDirect(\n",
    "        int(params['lstm1_units']), int(params['lstm2_units']),\n",
    "        int(params['dense1_units']), int(params['dense2_units']),\n",
    "        w\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epochs = int(params['epochs'])\n",
    "    best_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    \n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience: break\n",
    "            \n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def evaluate_combination_true_direct(params, full_dy, device_name):\n",
    "    \"\"\"\n",
    "    Evaluates params using Direct Strategy.\n",
    "    For each fold: Train 155 models (one per horizon).\n",
    "    \"\"\"\n",
    "    device = torch.device(device_name)\n",
    "    w = int(params['window_size'])\n",
    "    \n",
    "    # Make Splits on the full available history\n",
    "    splits = make_cv_splits(len(full_dy), VALID_SIZE, N_SPLITS)\n",
    "    \n",
    "    fold_rmses = []\n",
    "    \n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(splits):\n",
    "        # Data for this fold\n",
    "        fold_train_dy = full_dy[tr_idx]\n",
    "        fold_valid_dy = full_dy[va_idx]\n",
    "        \n",
    "        # Input for prediction is the Last window of the training set\n",
    "        last_window = fold_train_dy[-w:]\n",
    "        last_window_t = torch.tensor(last_window.reshape(1, w, 1), dtype=torch.float32).to(device)\n",
    "        \n",
    "        # List to store predictions for h=1 ถึง 155\n",
    "        fold_preds = []\n",
    "        \n",
    "        # Train 155 models for this fold\n",
    "        for h in range(1, VALID_SIZE + 1):\n",
    "            model_h = train_model_for_horizon(fold_train_dy, h, w, params, len(fold_train_dy)-1, device)\n",
    "            \n",
    "            if model_h is None:\n",
    "                fold_preds.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            model_h.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model_h(last_window_t).item()\n",
    "                fold_preds.append(pred)\n",
    "                        \n",
    "        rmse = np.sqrt(np.mean((fold_valid_dy - np.array(fold_preds))**2))\n",
    "        fold_rmses.append(rmse)\n",
    "        print(f\"   Fold {fold_idx+1}/{N_SPLITS} RMSE: {rmse:.4f}\")\n",
    "        \n",
    "    return np.mean(fold_rmses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# เริ่มดำเนินการและผลลัพธ์การ Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Combinations: 64\n",
      "Models per combo: 9920 (approx)\n",
      "   Fold 1/5 RMSE: 15.9542\n",
      "   Fold 1/5 RMSE: 16.6427\n",
      "   Fold 1/5 RMSE: 16.2784\n",
      "   Fold 1/5 RMSE: 16.3739\n",
      "   Fold 1/5 RMSE: 16.2115\n",
      "   Fold 1/5 RMSE: 16.5612\n",
      "   Fold 1/5 RMSE: 17.0200\n",
      "   Fold 1/5 RMSE: 15.8277\n",
      "   Fold 1/5 RMSE: 17.4752\n",
      "   Fold 1/5 RMSE: 16.3356\n",
      "   Fold 1/5 RMSE: 16.3870\n",
      "   Fold 1/5 RMSE: 16.1251\n",
      "   Fold 1/5 RMSE: 16.6337\n",
      "   Fold 1/5 RMSE: 16.3716\n",
      "   Fold 1/5 RMSE: 16.1016\n",
      "   Fold 1/5 RMSE: 15.1742\n",
      "   Fold 1/5 RMSE: 18.0038\n",
      "   Fold 1/5 RMSE: 17.2725\n",
      "   Fold 1/5 RMSE: 18.1351\n",
      "   Fold 1/5 RMSE: 17.8041\n",
      "   Fold 1/5 RMSE: 17.7704\n",
      "   Fold 1/5 RMSE: 17.5313\n",
      "   Fold 1/5 RMSE: 20.3833\n",
      "   Fold 1/5 RMSE: 17.6474\n",
      "   Fold 1/5 RMSE: 19.3362\n",
      "   Fold 1/5 RMSE: 18.1103\n",
      "   Fold 1/5 RMSE: 18.4423\n",
      "   Fold 1/5 RMSE: 18.2945\n",
      "   Fold 1/5 RMSE: 18.5608\n",
      "   Fold 1/5 RMSE: 17.6143\n",
      "   Fold 1/5 RMSE: 19.9640\n",
      "   Fold 1/5 RMSE: 17.6357\n",
      "   Fold 1/5 RMSE: 18.4967\n",
      "   Fold 1/5 RMSE: 17.6160\n",
      "   Fold 1/5 RMSE: 18.8856\n",
      "   Fold 1/5 RMSE: 17.6426\n",
      "   Fold 1/5 RMSE: 19.6905\n",
      "   Fold 1/5 RMSE: 17.5907\n",
      "   Fold 1/5 RMSE: 17.9292\n",
      "   Fold 1/5 RMSE: 18.3414\n",
      "   Fold 1/5 RMSE: 18.6060\n",
      "   Fold 1/5 RMSE: 19.2053\n",
      "   Fold 1/5 RMSE: 19.5179\n",
      "   Fold 1/5 RMSE: 18.5665\n",
      "   Fold 1/5 RMSE: 19.7106\n",
      "   Fold 1/5 RMSE: 18.6891\n",
      "   Fold 1/5 RMSE: 17.8225\n",
      "   Fold 1/5 RMSE: 20.0340\n",
      "   Fold 1/5 RMSE: 19.6259\n",
      "   Fold 1/5 RMSE: 18.3242\n",
      "   Fold 1/5 RMSE: 18.4221\n",
      "   Fold 1/5 RMSE: 18.6801\n",
      "   Fold 1/5 RMSE: 19.4266\n",
      "   Fold 1/5 RMSE: 19.5091\n",
      "   Fold 1/5 RMSE: 19.2437\n",
      "   Fold 1/5 RMSE: 19.4369\n",
      "   Fold 1/5 RMSE: 19.1383\n",
      "   Fold 1/5 RMSE: 19.6958\n",
      "   Fold 1/5 RMSE: 19.1349\n",
      "   Fold 1/5 RMSE: 18.3623\n",
      "   Fold 1/5 RMSE: 18.9429\n",
      "   Fold 1/5 RMSE: 17.7660\n",
      "   Fold 1/5 RMSE: 18.6506\n",
      "   Fold 1/5 RMSE: 18.9852\n",
      "   Fold 2/5 RMSE: 15.9272\n",
      "   Fold 2/5 RMSE: 15.3305\n",
      "   Fold 2/5 RMSE: 16.3301\n",
      "   Fold 2/5 RMSE: 16.1564\n",
      "   Fold 2/5 RMSE: 16.1784\n",
      "   Fold 2/5 RMSE: 15.9731\n",
      "   Fold 2/5 RMSE: 16.1201\n",
      "   Fold 2/5 RMSE: 15.8782\n",
      "   Fold 2/5 RMSE: 16.6120\n",
      "   Fold 2/5 RMSE: 16.0152\n",
      "   Fold 2/5 RMSE: 16.9419\n",
      "   Fold 2/5 RMSE: 15.5291\n",
      "   Fold 2/5 RMSE: 16.1928\n",
      "   Fold 2/5 RMSE: 16.8247\n",
      "   Fold 2/5 RMSE: 15.7243\n",
      "   Fold 2/5 RMSE: 15.5274\n",
      "   Fold 2/5 RMSE: 18.2473\n",
      "   Fold 2/5 RMSE: 16.9036\n",
      "   Fold 2/5 RMSE: 17.0035\n",
      "   Fold 2/5 RMSE: 16.8529\n",
      "   Fold 2/5 RMSE: 16.4552\n",
      "   Fold 2/5 RMSE: 16.9450\n",
      "   Fold 2/5 RMSE: 16.3126\n",
      "   Fold 2/5 RMSE: 16.8577\n",
      "   Fold 2/5 RMSE: 17.7195\n",
      "   Fold 2/5 RMSE: 17.8785\n",
      "   Fold 2/5 RMSE: 16.2612\n",
      "   Fold 2/5 RMSE: 16.3522\n",
      "   Fold 2/5 RMSE: 17.3967\n",
      "   Fold 2/5 RMSE: 18.6860\n",
      "   Fold 2/5 RMSE: 16.3857\n",
      "   Fold 2/5 RMSE: 17.3570\n",
      "   Fold 2/5 RMSE: 16.4520\n",
      "   Fold 2/5 RMSE: 19.0118\n",
      "   Fold 2/5 RMSE: 17.5167\n",
      "   Fold 2/5 RMSE: 18.1388\n",
      "   Fold 2/5 RMSE: 17.3778\n",
      "   Fold 2/5 RMSE: 17.6847\n",
      "   Fold 2/5 RMSE: 17.6360\n",
      "   Fold 2/5 RMSE: 17.2859\n",
      "   Fold 2/5 RMSE: 19.3960\n",
      "   Fold 2/5 RMSE: 17.3696\n",
      "   Fold 2/5 RMSE: 17.9222\n",
      "   Fold 2/5 RMSE: 17.7464\n",
      "   Fold 2/5 RMSE: 18.0356\n",
      "   Fold 2/5 RMSE: 17.0623\n",
      "   Fold 2/5 RMSE: 18.2973\n",
      "   Fold 2/5 RMSE: 18.5209\n",
      "   Fold 2/5 RMSE: 16.9018\n",
      "   Fold 2/5 RMSE: 18.8298\n",
      "   Fold 2/5 RMSE: 18.3566\n",
      "   Fold 2/5 RMSE: 18.4114\n",
      "   Fold 2/5 RMSE: 18.3457\n",
      "   Fold 2/5 RMSE: 17.7689\n",
      "   Fold 2/5 RMSE: 18.2523\n",
      "   Fold 2/5 RMSE: 16.8300\n",
      "   Fold 2/5 RMSE: 17.2920\n",
      "   Fold 3/5 RMSE: 18.3516\n",
      "   Fold 2/5 RMSE: 17.4075\n",
      "   Fold 2/5 RMSE: 17.7549\n",
      "   Fold 2/5 RMSE: 18.0563\n",
      "   Fold 2/5 RMSE: 17.9794\n",
      "   Fold 2/5 RMSE: 17.6648\n",
      "   Fold 3/5 RMSE: 17.3904\n",
      "   Fold 2/5 RMSE: 17.8242\n",
      "   Fold 2/5 RMSE: 17.4311\n",
      "   Fold 3/5 RMSE: 17.6259\n",
      "   Fold 3/5 RMSE: 18.0127\n",
      "   Fold 3/5 RMSE: 17.8385\n",
      "   Fold 3/5 RMSE: 17.7640\n",
      "   Fold 3/5 RMSE: 18.2941\n",
      "   Fold 3/5 RMSE: 17.4644\n",
      "   Fold 3/5 RMSE: 17.5813\n",
      "   Fold 3/5 RMSE: 17.5078\n",
      "   Fold 3/5 RMSE: 17.8977\n",
      "   Fold 3/5 RMSE: 17.3403\n",
      "   Fold 3/5 RMSE: 18.0461\n",
      "   Fold 3/5 RMSE: 17.1135\n",
      "   Fold 3/5 RMSE: 17.8134\n",
      "   Fold 3/5 RMSE: 18.0029\n",
      "   Fold 3/5 RMSE: 18.5056\n",
      "   Fold 3/5 RMSE: 18.9217\n",
      "   Fold 3/5 RMSE: 18.7297\n",
      "   Fold 3/5 RMSE: 18.1021\n",
      "   Fold 3/5 RMSE: 18.0792\n",
      "   Fold 3/5 RMSE: 19.4778\n",
      "   Fold 3/5 RMSE: 18.4548\n",
      "   Fold 3/5 RMSE: 18.0175\n",
      "   Fold 3/5 RMSE: 19.1771\n",
      "   Fold 3/5 RMSE: 19.2799\n",
      "   Fold 3/5 RMSE: 18.5320\n",
      "   Fold 3/5 RMSE: 18.8693\n",
      "   Fold 3/5 RMSE: 19.6093\n",
      "   Fold 3/5 RMSE: 18.1419\n",
      "   Fold 3/5 RMSE: 19.4855\n",
      "   Fold 3/5 RMSE: 20.3172\n",
      "   Fold 3/5 RMSE: 17.6650\n",
      "   Fold 3/5 RMSE: 16.8464\n",
      "   Fold 3/5 RMSE: 18.6969\n",
      "   Fold 3/5 RMSE: 19.8433\n",
      "   Fold 3/5 RMSE: 17.3779\n",
      "   Fold 3/5 RMSE: 18.2009\n",
      "   Fold 3/5 RMSE: 17.9537\n",
      "   Fold 3/5 RMSE: 20.0553\n",
      "   Fold 3/5 RMSE: 18.5001\n",
      "   Fold 3/5 RMSE: 19.9553\n",
      "   Fold 3/5 RMSE: 19.2525\n",
      "   Fold 3/5 RMSE: 20.6626\n",
      "   Fold 3/5 RMSE: 19.3192\n",
      "   Fold 3/5 RMSE: 18.8655\n",
      "   Fold 3/5 RMSE: 19.9158\n",
      "   Fold 3/5 RMSE: 20.2013\n",
      "   Fold 3/5 RMSE: 18.2931\n",
      "   Fold 3/5 RMSE: 19.7229\n",
      "   Fold 3/5 RMSE: 18.0843\n",
      "   Fold 3/5 RMSE: 19.5757\n",
      "   Fold 3/5 RMSE: 18.4254\n",
      "   Fold 3/5 RMSE: 19.2663\n",
      "   Fold 3/5 RMSE: 19.1177\n",
      "   Fold 3/5 RMSE: 20.7758\n",
      "   Fold 3/5 RMSE: 18.4662\n",
      "   Fold 3/5 RMSE: 19.5173\n",
      "   Fold 3/5 RMSE: 19.4896\n",
      "   Fold 3/5 RMSE: 20.2073\n",
      "   Fold 3/5 RMSE: 19.5701\n",
      "   Fold 3/5 RMSE: 18.2757\n",
      "   Fold 3/5 RMSE: 18.2878\n",
      "   Fold 3/5 RMSE: 18.6345\n",
      "   Fold 4/5 RMSE: 19.8794\n",
      "   Fold 4/5 RMSE: 20.9907\n",
      "   Fold 4/5 RMSE: 21.0386\n",
      "   Fold 4/5 RMSE: 20.3161\n",
      "   Fold 4/5 RMSE: 21.2203\n",
      "   Fold 4/5 RMSE: 21.5714\n",
      "   Fold 4/5 RMSE: 21.1058\n",
      "   Fold 4/5 RMSE: 20.9376\n",
      "   Fold 4/5 RMSE: 20.7387\n",
      "   Fold 4/5 RMSE: 21.7410\n",
      "   Fold 4/5 RMSE: 20.6878\n",
      "   Fold 4/5 RMSE: 20.6140\n",
      "   Fold 4/5 RMSE: 20.6959\n",
      "   Fold 4/5 RMSE: 20.9263\n",
      "   Fold 4/5 RMSE: 20.9122\n",
      "   Fold 4/5 RMSE: 20.7319\n",
      "   Fold 4/5 RMSE: 22.8976\n",
      "   Fold 4/5 RMSE: 22.3125\n",
      "   Fold 4/5 RMSE: 21.5185\n",
      "   Fold 4/5 RMSE: 22.6514\n",
      "   Fold 4/5 RMSE: 23.4341\n",
      "   Fold 4/5 RMSE: 21.3824\n",
      "   Fold 4/5 RMSE: 21.6982\n",
      "   Fold 4/5 RMSE: 22.1151\n",
      "   Fold 4/5 RMSE: 22.1143\n",
      "   Fold 4/5 RMSE: 21.9402\n",
      "   Fold 4/5 RMSE: 22.2836\n",
      "   Fold 4/5 RMSE: 21.9596\n",
      "   Fold 4/5 RMSE: 21.7040\n",
      "   Fold 4/5 RMSE: 21.5914\n",
      "   Fold 4/5 RMSE: 22.9235\n",
      "   Fold 4/5 RMSE: 23.0576\n",
      "   Fold 4/5 RMSE: 22.1403\n",
      "   Fold 4/5 RMSE: 25.2336\n",
      "   Fold 4/5 RMSE: 22.4890\n",
      "   Fold 4/5 RMSE: 22.2589\n",
      "   Fold 4/5 RMSE: 22.9103\n",
      "   Fold 4/5 RMSE: 21.5326\n",
      "   Fold 4/5 RMSE: 22.7923\n",
      "   Fold 4/5 RMSE: 22.6736\n",
      "   Fold 4/5 RMSE: 22.6344\n",
      "   Fold 4/5 RMSE: 22.1945\n",
      "   Fold 4/5 RMSE: 21.1553\n",
      "   Fold 4/5 RMSE: 21.7994\n",
      "   Fold 4/5 RMSE: 22.4939\n",
      "   Fold 4/5 RMSE: 22.0781\n",
      "   Fold 4/5 RMSE: 22.3447\n",
      "   Fold 4/5 RMSE: 22.2831\n",
      "   Fold 4/5 RMSE: 22.2076\n",
      "   Fold 4/5 RMSE: 22.0544\n",
      "   Fold 4/5 RMSE: 22.1671\n",
      "   Fold 4/5 RMSE: 22.6170\n",
      "   Fold 4/5 RMSE: 21.9221\n",
      "   Fold 4/5 RMSE: 21.9688\n",
      "   Fold 4/5 RMSE: 22.7429\n",
      "   Fold 4/5 RMSE: 22.0839\n",
      "   Fold 4/5 RMSE: 23.7915\n",
      "   Fold 4/5 RMSE: 23.0853\n",
      "   Fold 4/5 RMSE: 21.7520\n",
      "   Fold 4/5 RMSE: 21.1725\n",
      "   Fold 5/5 RMSE: 23.5638\n",
      "   Fold 4/5 RMSE: 22.1829\n",
      "   Fold 4/5 RMSE: 22.0262\n",
      "   Fold 4/5 RMSE: 21.7537\n",
      "   Fold 4/5 RMSE: 21.8533\n",
      "   Fold 5/5 RMSE: 22.2123\n",
      "   Fold 5/5 RMSE: 24.0499\n",
      "   Fold 5/5 RMSE: 22.8369\n",
      "   Fold 5/5 RMSE: 21.9229\n",
      "   Fold 5/5 RMSE: 22.4449\n",
      "   Fold 5/5 RMSE: 23.1956\n",
      "   Fold 5/5 RMSE: 23.1146\n",
      "   Fold 5/5 RMSE: 22.9875\n",
      "   Fold 5/5 RMSE: 23.0663\n",
      "   Fold 5/5 RMSE: 22.5710\n",
      "   Fold 5/5 RMSE: 22.1621\n",
      "   Fold 5/5 RMSE: 23.1118\n",
      "   Fold 5/5 RMSE: 22.9657\n",
      "   Fold 5/5 RMSE: 21.7217\n",
      "   Fold 5/5 RMSE: 23.3008\n",
      "   Fold 5/5 RMSE: 22.6773\n",
      "   Fold 5/5 RMSE: 23.2691\n",
      "   Fold 5/5 RMSE: 23.5223\n",
      "   Fold 5/5 RMSE: 23.3689\n",
      "   Fold 5/5 RMSE: 23.8958\n",
      "   Fold 5/5 RMSE: 23.0602\n",
      "   Fold 5/5 RMSE: 23.8423\n",
      "   Fold 5/5 RMSE: 25.1278\n",
      "   Fold 5/5 RMSE: 23.5583\n",
      "   Fold 5/5 RMSE: 24.1786\n",
      "   Fold 5/5 RMSE: 24.4185\n",
      "   Fold 5/5 RMSE: 24.2102\n",
      "   Fold 5/5 RMSE: 24.6398\n",
      "   Fold 5/5 RMSE: 23.0660\n",
      "   Fold 5/5 RMSE: 22.3280\n",
      "   Fold 5/5 RMSE: 23.8542\n",
      "   Fold 5/5 RMSE: 22.7633\n",
      "   Fold 5/5 RMSE: 22.7450\n",
      "   Fold 5/5 RMSE: 24.6801\n",
      "   Fold 5/5 RMSE: 23.7625\n",
      "   Fold 5/5 RMSE: 24.4741\n",
      "   Fold 5/5 RMSE: 24.2632\n",
      "   Fold 5/5 RMSE: 23.0576\n",
      "   Fold 5/5 RMSE: 23.2865\n",
      "   Fold 5/5 RMSE: 23.6973\n",
      "   Fold 5/5 RMSE: 24.4262\n",
      "   Fold 5/5 RMSE: 24.0617\n",
      "   Fold 5/5 RMSE: 23.3810\n",
      "   Fold 5/5 RMSE: 24.7299\n",
      "   Fold 5/5 RMSE: 24.8598\n",
      "   Fold 5/5 RMSE: 23.5715\n",
      "   Fold 5/5 RMSE: 24.5290\n",
      "   Fold 5/5 RMSE: 24.2684\n",
      "   Fold 5/5 RMSE: 23.7230\n",
      "   Fold 5/5 RMSE: 23.4421\n",
      "   Fold 5/5 RMSE: 23.4903\n",
      "   Fold 5/5 RMSE: 23.9645\n",
      "   Fold 5/5 RMSE: 23.5562\n",
      "   Fold 5/5 RMSE: 24.9458\n",
      "   Fold 5/5 RMSE: 24.1732\n",
      "   Fold 5/5 RMSE: 23.1650\n",
      "   Fold 5/5 RMSE: 25.4734\n",
      "   Fold 5/5 RMSE: 24.8321\n",
      "   Fold 5/5 RMSE: 22.5802\n",
      "   Fold 5/5 RMSE: 23.4902\n",
      "   Fold 5/5 RMSE: 23.3539\n",
      "   Fold 5/5 RMSE: 23.9458\n",
      "   Fold 5/5 RMSE: 23.8307\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.6640\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.4950\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 21.4849\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 21.1477\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.7373\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.7165\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.8269\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.7769\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.7718\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.8450\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.6954\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.7006\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.8676\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.0471\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.6367\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.7943\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.6129\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.9069\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.1379\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.2560\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.9670\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.5886\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.5653\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.4087\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 19.0570\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.4276\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.6534\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.1039\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 18.8788\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 18.7823\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.5683\n",
      "Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.8250\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 19.6359\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.6609\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.7913\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.4495\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 19.9651\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.7548\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 21.1120\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.3223\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.0402\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.3294\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 21.0200\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.4270\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.1687\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.6523\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.1780\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 32, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.1470\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 19.6480\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.4862\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 19.9828\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.9322\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.0772\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.4522\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.5098\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 32, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.1561\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 19.9098\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.4439\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.3934\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.9676\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.0791\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}, CV-RMSE: 19.8037\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 10}, CV-RMSE: 20.4654\n",
      "Params: {'lstm1_units': 100, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 64, 'learning_rate': 0.01, 'epochs': 50, 'window_size': 20}, CV-RMSE: 20.0364\n",
      "\n",
      "Grid Search Complete.\n",
      "Best Params: {'lstm1_units': 50, 'lstm2_units': 64, 'dense1_units': 64, 'dense2_units': 32, 'learning_rate': 0.001, 'epochs': 50, 'window_size': 20}\n",
      "Best RMSE: 18.427604627421193\n"
     ]
    }
   ],
   "source": [
    "# 6. Execution\n",
    "\n",
    "hyper_grid = {\n",
    "    'lstm1_units': [50,100],\n",
    "    'lstm2_units': [32,64],\n",
    "    'dense1_units': [32,64],\n",
    "    'dense2_units': [32,64],\n",
    "    'learning_rate': [0.001,0.01],\n",
    "    'epochs': [50],\n",
    "    'window_size': [10,20]\n",
    "}\n",
    "\n",
    "keys, values = zip(*hyper_grid.items())\n",
    "combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "print(f\"Total Combinations: {len(combinations)}\")\n",
    "print(f\"Models per combo: {N_JOBS * VALID_SIZE} (approx)\")\n",
    "\n",
    "# Parallel Execution\n",
    "N_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "\n",
    "def safe_evaluate(params, worker_id):\n",
    "    if N_GPUS > 0:\n",
    "        gpu_id = worker_id % N_GPUS\n",
    "        device_str = f'cuda:{gpu_id}'\n",
    "    else:\n",
    "        device_str = 'cpu'\n",
    "    return evaluate_combination_true_direct(params, dy_train_full, device_str)\n",
    "\n",
    "results = Parallel(n_jobs=N_JOBS)(\n",
    "    delayed(safe_evaluate)(params, i) \n",
    "    for i, params in enumerate(combinations)\n",
    ")\n",
    "\n",
    "# Process Results\n",
    "best_score = float('inf')\n",
    "best_params = None\n",
    "\n",
    "for i, score in enumerate(results):\n",
    "    params = combinations[i]\n",
    "    print(f\"Params: {params}, CV-RMSE: {score:.4f}\")\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\nGrid Search Complete.\")\n",
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best RMSE:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
